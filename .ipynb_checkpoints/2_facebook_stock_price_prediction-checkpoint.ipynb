{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACEBOOK STOCKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FaceBook's Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB -- FaceBook's ticker\n",
    "facebook = yf.Ticker('FB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = facebook.history(period='max', interval='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-05-18</th>\n",
       "      <td>42.049999</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.230000</td>\n",
       "      <td>573576400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-21</th>\n",
       "      <td>36.529999</td>\n",
       "      <td>36.660000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.029999</td>\n",
       "      <td>168192700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-22</th>\n",
       "      <td>32.610001</td>\n",
       "      <td>33.590000</td>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>101786600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-23</th>\n",
       "      <td>31.370001</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>31.360001</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>73600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-24</th>\n",
       "      <td>32.950001</td>\n",
       "      <td>33.209999</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>33.029999</td>\n",
       "      <td>50237200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30</th>\n",
       "      <td>276.029999</td>\n",
       "      <td>277.700012</td>\n",
       "      <td>271.010010</td>\n",
       "      <td>276.970001</td>\n",
       "      <td>16693300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>279.160004</td>\n",
       "      <td>289.299988</td>\n",
       "      <td>278.959991</td>\n",
       "      <td>286.549988</td>\n",
       "      <td>20777900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-02</th>\n",
       "      <td>285.359985</td>\n",
       "      <td>291.779999</td>\n",
       "      <td>280.829987</td>\n",
       "      <td>287.519989</td>\n",
       "      <td>17361600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-03</th>\n",
       "      <td>286.250000</td>\n",
       "      <td>286.649994</td>\n",
       "      <td>281.070007</td>\n",
       "      <td>281.850006</td>\n",
       "      <td>12921700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-04</th>\n",
       "      <td>280.299988</td>\n",
       "      <td>283.459991</td>\n",
       "      <td>279.299988</td>\n",
       "      <td>279.700012</td>\n",
       "      <td>10846900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2152 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close     Volume  \\\n",
       "Date                                                                    \n",
       "2012-05-18   42.049999   45.000000   38.000000   38.230000  573576400   \n",
       "2012-05-21   36.529999   36.660000   33.000000   34.029999  168192700   \n",
       "2012-05-22   32.610001   33.590000   30.940001   31.000000  101786600   \n",
       "2012-05-23   31.370001   32.500000   31.360001   32.000000   73600000   \n",
       "2012-05-24   32.950001   33.209999   31.770000   33.029999   50237200   \n",
       "...                ...         ...         ...         ...        ...   \n",
       "2020-11-30  276.029999  277.700012  271.010010  276.970001   16693300   \n",
       "2020-12-01  279.160004  289.299988  278.959991  286.549988   20777900   \n",
       "2020-12-02  285.359985  291.779999  280.829987  287.519989   17361600   \n",
       "2020-12-03  286.250000  286.649994  281.070007  281.850006   12921700   \n",
       "2020-12-04  280.299988  283.459991  279.299988  279.700012   10846900   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "Date                                 \n",
       "2012-05-18          0             0  \n",
       "2012-05-21          0             0  \n",
       "2012-05-22          0             0  \n",
       "2012-05-23          0             0  \n",
       "2012-05-24          0             0  \n",
       "...               ...           ...  \n",
       "2020-11-30          0             0  \n",
       "2020-12-01          0             0  \n",
       "2020-12-02          0             0  \n",
       "2020-12-03          0             0  \n",
       "2020-12-04          0             0  \n",
       "\n",
       "[2152 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function that uses TimeseriesGenerator class to generate the training set with dividends info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_series(data, value_num):\n",
    "    close = data['Close']\n",
    "    dividends = data['Dividends']\n",
    "    tsg = TimeseriesGenerator(close, close,\n",
    "                              length=value_num,\n",
    "                              batch_size=len(close))\n",
    "    global_index = value_num\n",
    "    i, t = tsg[0]\n",
    "    has_dividends = np.zeros(len(i))\n",
    "    for b_row in range(len(t)):\n",
    "        assert(abs(t[b_row] - close[global_index]) <= 0.001)\n",
    "        has_dividends[b_row] = dividends[global_index] > 0            \n",
    "        global_index += 1\n",
    "    return np.concatenate((i, np.transpose([has_dividends])),\n",
    "                           axis=1), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_series(history, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing MinMax normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_min = history.min()\n",
    "normalized_h = (history - h_min) / (history.max() - h_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_series(normalized_h, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creates a neural network with a specified number of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n):\n",
    "    m = models.Sequential()\n",
    "    m.add(layers.Dense(64, activation='relu', input_shape=(n+1,)))\n",
    "    m.add(layers.Dense(64, activation='relu'))\n",
    "    m.add(layers.Dense(1))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = inputs[:-1000]\n",
    "val_inputs = inputs[-1000:]\n",
    "train_targets = targets[:-1000]\n",
    "val_targets = targets[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_inputs(data, start, end, epochs):\n",
    "    models = {}\n",
    "    for inputs in range(start, end+1):\n",
    "        print('Using {} inputs'.format(inputs))\n",
    "        model_inputs, targets = generate_series(data, inputs)\n",
    "        \n",
    "        train_inputs = model_inputs[:-1000]\n",
    "        val_inputs = model_inputs[-1000:]\n",
    "        train_targets = targets[:-1000]\n",
    "        val_targets = targets[-1000:]\n",
    "        \n",
    "        m = create_model(inputs)\n",
    "        print('Training')\n",
    "        m.compile(optimizer='adam', loss='mse') \n",
    "        h = m.fit(train_inputs, train_targets,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=32,\n",
    "                  validation_data=(val_inputs, val_targets))\n",
    "        model_info = {'model': m, 'history': h.history}\n",
    "        models[inputs] = model_info\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.0061 - val_loss: 0.0015\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.7020e-05 - val_loss: 4.4644e-04\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 3.4726e-05 - val_loss: 4.9577e-04\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.1561e-05 - val_loss: 4.1621e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.2517e-05 - val_loss: 4.3014e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 3.0838e-05 - val_loss: 4.3542e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 2.9783e-05 - val_loss: 5.4781e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 3.3435e-05 - val_loss: 4.4974e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 2.9846e-05 - val_loss: 5.3134e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 3.1772e-05 - val_loss: 6.3782e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.1889e-05 - val_loss: 4.9489e-04\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 3.1395e-05 - val_loss: 5.9469e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 3.0146e-05 - val_loss: 5.0028e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 3.0761e-05 - val_loss: 4.7436e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 3.0563e-05 - val_loss: 5.7894e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 3.0145e-05 - val_loss: 3.7623e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 3.0905e-05 - val_loss: 4.3855e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 2.9128e-05 - val_loss: 4.4316e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 2.8553e-05 - val_loss: 4.5596e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 2.9094e-05 - val_loss: 3.9046e-04\n",
      "Using 3 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.0083 - val_loss: 0.0098\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.1386e-04 - val_loss: 6.0287e-04\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.6822e-05 - val_loss: 4.5622e-04\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.6511e-05 - val_loss: 4.9542e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.5667e-05 - val_loss: 5.0953e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.4473e-05 - val_loss: 4.7108e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 3.3088e-05 - val_loss: 4.9940e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 3.3908e-05 - val_loss: 4.7775e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.3761e-05 - val_loss: 4.9562e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 3.3285e-05 - val_loss: 5.6084e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 3.0819e-05 - val_loss: 5.2848e-04\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.4706e-05 - val_loss: 6.6463e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 3.5496e-05 - val_loss: 5.6813e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 3.1106e-05 - val_loss: 5.5866e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 2.8803e-05 - val_loss: 4.9421e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 3.0454e-05 - val_loss: 4.9823e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 2.9964e-05 - val_loss: 5.6034e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 2.8172e-05 - val_loss: 5.9214e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 3.0550e-05 - val_loss: 5.5161e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 2.8947e-05 - val_loss: 6.8186e-04\n",
      "Using 4 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.0049 - val_loss: 0.0071\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 9.9171e-05 - val_loss: 4.4478e-04\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 4.2067e-05 - val_loss: 3.5933e-04\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 4.0369e-05 - val_loss: 4.5890e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 4.2991e-05 - val_loss: 3.2519e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.7596e-05 - val_loss: 3.2625e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.6727e-05 - val_loss: 3.4323e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 3.5456e-05 - val_loss: 3.5325e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 3.5744e-05 - val_loss: 3.0314e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 3.8657e-05 - val_loss: 2.9283e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 3.6419e-05 - val_loss: 3.6643e-04\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 3.3956e-05 - val_loss: 3.2441e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 3.3345e-05 - val_loss: 3.0538e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 3.2793e-05 - val_loss: 3.1135e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.2222e-05 - val_loss: 3.2615e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.3005e-05 - val_loss: 3.0669e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.4352e-05 - val_loss: 3.1489e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.4465e-05 - val_loss: 2.7783e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.4403e-05 - val_loss: 3.9582e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.3279e-05 - val_loss: 3.5966e-04\n",
      "Using 5 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 7.4258e-05 - val_loss: 5.1601e-04\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 5.6235e-05 - val_loss: 5.1294e-04\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 5.2596e-05 - val_loss: 4.5730e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 5.0635e-05 - val_loss: 5.0114e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 4.8220e-05 - val_loss: 4.9171e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 4.4704e-05 - val_loss: 4.3615e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 4.3112e-05 - val_loss: 4.1511e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 4.0653e-05 - val_loss: 4.5005e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.8820e-05 - val_loss: 3.8416e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 3.7536e-05 - val_loss: 4.1091e-04\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.7929e-05 - val_loss: 4.4573e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.8236e-05 - val_loss: 3.2273e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.8141e-05 - val_loss: 4.2067e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.5024e-05 - val_loss: 3.2316e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.9238e-05 - val_loss: 3.6223e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.6059e-05 - val_loss: 3.1131e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 3.3978e-05 - val_loss: 6.2916e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.7158e-05 - val_loss: 3.3175e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 3.1773e-05 - val_loss: 3.6815e-04\n",
      "Using 6 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0211 - val_loss: 5.0342e-04\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 7.6349e-05 - val_loss: 4.4155e-04\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 6.4161e-05 - val_loss: 6.8881e-04\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 6.2168e-05 - val_loss: 4.0929e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 5.5443e-05 - val_loss: 3.8646e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8333e-05 - val_loss: 4.2903e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 5.2205e-05 - val_loss: 3.8406e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 4.9449e-05 - val_loss: 3.6009e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.8279e-05 - val_loss: 3.4273e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.9145e-05 - val_loss: 3.7402e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.6368e-05 - val_loss: 3.2729e-04\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.3700e-05 - val_loss: 3.5775e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 4.8754e-05 - val_loss: 3.1418e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.1463e-05 - val_loss: 3.0947e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 3.8908e-05 - val_loss: 3.2038e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 3.9263e-05 - val_loss: 2.9553e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 3.8192e-05 - val_loss: 3.1332e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 4.3175e-05 - val_loss: 4.1335e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 4.7533e-05 - val_loss: 2.8378e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 3.5356e-05 - val_loss: 2.9006e-04\n",
      "Using 7 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.0054 - val_loss: 0.0019\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 8.2922e-05 - val_loss: 8.8416e-04\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 5.3696e-05 - val_loss: 6.3661e-04\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 5.1332e-05 - val_loss: 5.4532e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 4.8825e-05 - val_loss: 4.5876e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 4.6361e-05 - val_loss: 4.1259e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 4.8696e-05 - val_loss: 5.4993e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 4.3783e-05 - val_loss: 3.9496e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 4.4274e-05 - val_loss: 4.1934e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 4.1333e-05 - val_loss: 4.0082e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 4.7497e-05 - val_loss: 5.0855e-04\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 4.4604e-05 - val_loss: 4.3368e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 4.0893e-05 - val_loss: 4.7225e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 4.3506e-05 - val_loss: 3.7160e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 3.8593e-05 - val_loss: 3.7121e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 4.0243e-05 - val_loss: 3.4581e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 3.9714e-05 - val_loss: 4.1452e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.0188e-05 - val_loss: 3.4098e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 3.9918e-05 - val_loss: 3.2333e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 4.5392e-05 - val_loss: 3.5395e-04\n",
      "Using 8 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.0066 - val_loss: 0.0033\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.4203e-04 - val_loss: 0.0015\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 6.8771e-05 - val_loss: 0.0012\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 5.9720e-05 - val_loss: 8.6910e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 5.7926e-05 - val_loss: 8.3318e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 5.6790e-05 - val_loss: 9.0239e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 5.5015e-05 - val_loss: 8.6207e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 5.2743e-05 - val_loss: 8.4931e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 5.0544e-05 - val_loss: 7.7349e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 5.1446e-05 - val_loss: 8.9470e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 4.7544e-05 - val_loss: 0.0010\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 4.4047e-05 - val_loss: 6.8663e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 4.5070e-05 - val_loss: 8.1171e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 3.9458e-05 - val_loss: 8.4393e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 3.7978e-05 - val_loss: 7.3061e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 3.8956e-05 - val_loss: 9.8959e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 4.0465e-05 - val_loss: 5.9909e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 3.5623e-05 - val_loss: 7.5637e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 3.5609e-05 - val_loss: 7.1328e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 3.7429e-05 - val_loss: 7.6625e-04\n",
      "Using 9 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.0068 - val_loss: 0.0041\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.0296e-04 - val_loss: 9.0741e-04\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 7.2581e-05 - val_loss: 8.3277e-04\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 6.6789e-05 - val_loss: 6.2673e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 6.1601e-05 - val_loss: 6.1341e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 6.3182e-05 - val_loss: 5.7509e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.5961e-05 - val_loss: 6.2038e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4706e-05 - val_loss: 5.7192e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.1577e-05 - val_loss: 7.8656e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.4283e-05 - val_loss: 7.0098e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 4.4224e-05 - val_loss: 6.1331e-04\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.1340e-05 - val_loss: 4.2116e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.7200e-05 - val_loss: 4.6288e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.9066e-05 - val_loss: 4.5989e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.6069e-05 - val_loss: 4.5987e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.6781e-05 - val_loss: 5.0389e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.6024e-05 - val_loss: 3.5192e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.6030e-05 - val_loss: 3.7507e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.6767e-05 - val_loss: 5.6061e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.5592e-05 - val_loss: 4.2558e-04\n",
      "Using 10 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 6.8416e-04 - val_loss: 7.9596e-04\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 8.5584e-05 - val_loss: 6.0494e-04\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 6.3758e-05 - val_loss: 5.8481e-04\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.4848e-05 - val_loss: 4.2061e-04\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.9148e-05 - val_loss: 4.1147e-04\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.5693e-05 - val_loss: 4.0034e-04\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 4.2789e-05 - val_loss: 4.1221e-04\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.3013e-05 - val_loss: 3.4723e-04\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.3816e-05 - val_loss: 7.7929e-04\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.4801e-05 - val_loss: 3.5829e-04\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 5.1836e-05 - val_loss: 3.3951e-04\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.1349e-05 - val_loss: 3.4192e-04\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 3.9313e-05 - val_loss: 4.0320e-04\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.0336e-05 - val_loss: 3.4148e-04\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.1403e-05 - val_loss: 3.3028e-04\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.0964e-05 - val_loss: 3.8401e-04\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.4430e-05 - val_loss: 4.6396e-04\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 4.2531e-05 - val_loss: 3.6784e-04\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 4.5213e-05 - val_loss: 3.0488e-04\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 3.7502e-05 - val_loss: 3.0215e-04\n"
     ]
    }
   ],
   "source": [
    "trained_models = select_inputs(normalized_h, 2, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trained_models[2]['model'].predict(val_targets))\n",
    "model = trained_models[2]['model']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = {}\n",
    "for k, v in trained_models.items():\n",
    "    train_history = v['history']\n",
    "    loss = train_history['loss'][-1]\n",
    "    val_loss = train_history['val_loss'][-1]\n",
    "    model_stats[k] = {'inputs': k, 'loss': loss, 'val_loss': val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {'inputs': 2,\n",
       "  'loss': 2.9093720513628796e-05,\n",
       "  'val_loss': 0.00039046345045790076},\n",
       " 3: {'inputs': 3,\n",
       "  'loss': 2.894677709264215e-05,\n",
       "  'val_loss': 0.0006818611291237175},\n",
       " 4: {'inputs': 4,\n",
       "  'loss': 3.327882586745545e-05,\n",
       "  'val_loss': 0.00035965978167951107},\n",
       " 5: {'inputs': 5,\n",
       "  'loss': 3.177298640366644e-05,\n",
       "  'val_loss': 0.00036815073690377176},\n",
       " 6: {'inputs': 6,\n",
       "  'loss': 3.535562791512348e-05,\n",
       "  'val_loss': 0.0002900561667047441},\n",
       " 7: {'inputs': 7,\n",
       "  'loss': 4.5392007450573146e-05,\n",
       "  'val_loss': 0.0003539513854775578},\n",
       " 8: {'inputs': 8,\n",
       "  'loss': 3.7428599171107635e-05,\n",
       "  'val_loss': 0.0007662465795874596},\n",
       " 9: {'inputs': 9,\n",
       "  'loss': 3.559185643098317e-05,\n",
       "  'val_loss': 0.0004255782987456769},\n",
       " 10: {'inputs': 10,\n",
       "  'loss': 3.750169344129972e-05,\n",
       "  'val_loss': 0.00030215061269700527}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f77a6f79160>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = []\n",
    "indices = []\n",
    "for k, v in model_stats.items():\n",
    "    indices.append(k)\n",
    "    val_loss.append(v['val_loss'])\n",
    "plt.plot(indices, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 17.84174237409703\n",
      "3 17.925134562978133\n",
      "4 17.832926980039122\n",
      "5 17.835356921640184\n",
      "6 17.813007817218885\n",
      "7 17.83129335119053\n",
      "8 17.949283991539506\n",
      "9 17.851791541524694\n",
      "10 17.81646900582278\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "close_min = history['Close'].min()\n",
    "close_max = history['Close'].max()\n",
    "for k in model_stats:\n",
    "    e = ((close_max - close_min) * model_stats[k]['val_loss'] + close_min)\n",
    "    vals.append(e)\n",
    "    print(k, e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
