{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apple's Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAPL -- Apple's ticker\n",
    "apple = yf.Ticker('AAPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = apple.history(period='max', interval='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-12-12</th>\n",
       "      <td>0.100266</td>\n",
       "      <td>0.100702</td>\n",
       "      <td>0.100266</td>\n",
       "      <td>0.100266</td>\n",
       "      <td>469033600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-15</th>\n",
       "      <td>0.095470</td>\n",
       "      <td>0.095470</td>\n",
       "      <td>0.095035</td>\n",
       "      <td>0.095035</td>\n",
       "      <td>175884800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-16</th>\n",
       "      <td>0.088495</td>\n",
       "      <td>0.088495</td>\n",
       "      <td>0.088059</td>\n",
       "      <td>0.088059</td>\n",
       "      <td>105728000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-17</th>\n",
       "      <td>0.090239</td>\n",
       "      <td>0.090675</td>\n",
       "      <td>0.090239</td>\n",
       "      <td>0.090239</td>\n",
       "      <td>86441600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-18</th>\n",
       "      <td>0.092855</td>\n",
       "      <td>0.093291</td>\n",
       "      <td>0.092855</td>\n",
       "      <td>0.092855</td>\n",
       "      <td>73449600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30</th>\n",
       "      <td>116.970001</td>\n",
       "      <td>120.970001</td>\n",
       "      <td>116.809998</td>\n",
       "      <td>119.050003</td>\n",
       "      <td>169410200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>121.010002</td>\n",
       "      <td>123.470001</td>\n",
       "      <td>120.010002</td>\n",
       "      <td>122.720001</td>\n",
       "      <td>128166800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-02</th>\n",
       "      <td>122.019997</td>\n",
       "      <td>123.370003</td>\n",
       "      <td>120.889999</td>\n",
       "      <td>123.080002</td>\n",
       "      <td>89004200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-03</th>\n",
       "      <td>123.519997</td>\n",
       "      <td>123.779999</td>\n",
       "      <td>122.209999</td>\n",
       "      <td>122.940002</td>\n",
       "      <td>78967600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-04</th>\n",
       "      <td>122.599998</td>\n",
       "      <td>122.860001</td>\n",
       "      <td>121.519997</td>\n",
       "      <td>122.250000</td>\n",
       "      <td>78133200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10081 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close     Volume  \\\n",
       "Date                                                                    \n",
       "1980-12-12    0.100266    0.100702    0.100266    0.100266  469033600   \n",
       "1980-12-15    0.095470    0.095470    0.095035    0.095035  175884800   \n",
       "1980-12-16    0.088495    0.088495    0.088059    0.088059  105728000   \n",
       "1980-12-17    0.090239    0.090675    0.090239    0.090239   86441600   \n",
       "1980-12-18    0.092855    0.093291    0.092855    0.092855   73449600   \n",
       "...                ...         ...         ...         ...        ...   \n",
       "2020-11-30  116.970001  120.970001  116.809998  119.050003  169410200   \n",
       "2020-12-01  121.010002  123.470001  120.010002  122.720001  128166800   \n",
       "2020-12-02  122.019997  123.370003  120.889999  123.080002   89004200   \n",
       "2020-12-03  123.519997  123.779999  122.209999  122.940002   78967600   \n",
       "2020-12-04  122.599998  122.860001  121.519997  122.250000   78133200   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "Date                                 \n",
       "1980-12-12        0.0           0.0  \n",
       "1980-12-15        0.0           0.0  \n",
       "1980-12-16        0.0           0.0  \n",
       "1980-12-17        0.0           0.0  \n",
       "1980-12-18        0.0           0.0  \n",
       "...               ...           ...  \n",
       "2020-11-30        0.0           0.0  \n",
       "2020-12-01        0.0           0.0  \n",
       "2020-12-02        0.0           0.0  \n",
       "2020-12-03        0.0           0.0  \n",
       "2020-12-04        0.0           0.0  \n",
       "\n",
       "[10081 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function that uses TimeseriesGenerator class to generate the training set with dividends info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_series(data, value_num):\n",
    "    close = data['Close']\n",
    "    dividends = data['Dividends']\n",
    "    tsg = TimeseriesGenerator(close, close,\n",
    "                              length=value_num,\n",
    "                              batch_size=len(close))\n",
    "    global_index = value_num\n",
    "    i, t = tsg[0]\n",
    "    has_dividends = np.zeros(len(i))\n",
    "    for b_row in range(len(t)):\n",
    "        assert(abs(t[b_row] - close[global_index]) <= 0.001)\n",
    "        has_dividends[b_row] = dividends[global_index] > 0            \n",
    "        global_index += 1\n",
    "    return np.concatenate((i, np.transpose([has_dividends])),\n",
    "                           axis=1), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_series(history, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing MinMax normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_min = history.min()\n",
    "normalized_h = (history - h_min) / (history.max() - h_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_series(normalized_h, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creates a neural network with a specified number of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n):\n",
    "    m = models.Sequential()\n",
    "    m.add(layers.Dense(64, activation='relu', input_shape=(n+1,)))\n",
    "    m.add(layers.Dense(64, activation='relu'))\n",
    "    m.add(layers.Dense(1))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = inputs[:-1000]\n",
    "val_inputs = inputs[-1000:]\n",
    "train_targets = targets[:-1000]\n",
    "val_targets = targets[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_inputs(data, start, end, epochs):\n",
    "    models = {}\n",
    "    for inputs in range(start, end+1):\n",
    "        print('Using {} inputs'.format(inputs))\n",
    "        model_inputs, targets = generate_series(data, inputs)\n",
    "        \n",
    "        train_inputs = model_inputs[:-1000]\n",
    "        val_inputs = model_inputs[-1000:]\n",
    "        train_targets = targets[:-1000]\n",
    "        val_targets = targets[-1000:]\n",
    "        \n",
    "        m = create_model(inputs)\n",
    "        print('Training')\n",
    "        m.compile(optimizer='adam', loss='mse') \n",
    "        h = m.fit(train_inputs, train_targets,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=32,\n",
    "                  validation_data=(val_inputs, val_targets))\n",
    "        model_info = {'model': m, 'history': h.history}\n",
    "        models[inputs] = model_info\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.2847e-04 - val_loss: 0.0112\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 7.9040e-06 - val_loss: 0.0103\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.6379e-06 - val_loss: 0.0101\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5415e-06 - val_loss: 0.0105\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 1.8382e-06 - val_loss: 0.0098\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.9635e-06 - val_loss: 0.0099\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.7454e-06 - val_loss: 0.0107\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 7.1137e-06 - val_loss: 0.0089\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.5461e-06 - val_loss: 0.0090\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.9883e-06 - val_loss: 0.0085\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.8386e-06 - val_loss: 0.0084\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 4.7152e-06 - val_loss: 0.0076\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.4003e-06 - val_loss: 0.0073\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 1.9309e-06 - val_loss: 0.0072\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.5418e-06 - val_loss: 0.0071\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 1.8986e-06 - val_loss: 0.0073\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.4568e-06 - val_loss: 0.0077\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.0582e-06 - val_loss: 0.0067\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 1.7706e-06 - val_loss: 0.0056\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.1412e-06 - val_loss: 0.0055\n",
      "Using 3 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 7.4422e-05 - val_loss: 0.0025\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.4262e-06 - val_loss: 0.0026\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.9711e-06 - val_loss: 0.0025\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.3833e-06 - val_loss: 0.0025\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.8001e-06 - val_loss: 0.0028\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 4.2795e-06 - val_loss: 0.0031\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.0997e-06 - val_loss: 0.0032\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.5648e-06 - val_loss: 0.0026\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.2706e-06 - val_loss: 0.0030\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.3468e-06 - val_loss: 0.0024\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.5794e-06 - val_loss: 0.0024\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.0618e-06 - val_loss: 0.0029\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.0439e-06 - val_loss: 0.0030\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.1170e-06 - val_loss: 0.0029\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.2195e-06 - val_loss: 0.0033\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.9839e-06 - val_loss: 0.0026\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.8654e-06 - val_loss: 0.0029\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 1.9230e-06 - val_loss: 0.0025\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 4.1543e-06 - val_loss: 0.0021\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.0418e-06 - val_loss: 0.0020\n",
      "Using 4 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 5.1360e-05 - val_loss: 0.0010\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 4.0867e-06 - val_loss: 0.0012\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.3268e-05 - val_loss: 0.0013\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.8490e-06 - val_loss: 9.8952e-04\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.3210e-06 - val_loss: 8.9021e-04\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.0806e-06 - val_loss: 0.0011\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.5669e-06 - val_loss: 0.0010\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.4492e-06 - val_loss: 9.0046e-04\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.8329e-06 - val_loss: 0.0012\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 6.3683e-06 - val_loss: 0.0016\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.8932e-06 - val_loss: 0.0012\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.1874e-06 - val_loss: 0.0010\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.2937e-06 - val_loss: 0.0011\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.7345e-06 - val_loss: 9.5793e-04\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.5061e-06 - val_loss: 0.0010\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.3450e-06 - val_loss: 5.4613e-04\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.3109e-06 - val_loss: 8.2849e-04\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.9828e-06 - val_loss: 6.6893e-04\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.5230e-06 - val_loss: 7.3314e-04\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.9772e-06 - val_loss: 7.4393e-04\n",
      "Using 5 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 6.9168e-05 - val_loss: 0.0015\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 6.3104e-06 - val_loss: 0.0011\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 7.8135e-06 - val_loss: 0.0016\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.7189e-06 - val_loss: 0.0018\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 8.1545e-06 - val_loss: 0.0020\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 5.0346e-06 - val_loss: 0.0018\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.7483e-06 - val_loss: 0.0021\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.8817e-06 - val_loss: 0.0016\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.5250e-06 - val_loss: 0.0025\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.9874e-06 - val_loss: 0.0020\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.0206e-06 - val_loss: 0.0021\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.3336e-06 - val_loss: 0.0020\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 8.8993e-06 - val_loss: 0.0017\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 4.4821e-06 - val_loss: 0.0013\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.2427e-06 - val_loss: 0.0013\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.0976e-06 - val_loss: 0.0017\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.5818e-06 - val_loss: 9.8788e-04\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.1770e-06 - val_loss: 0.0013\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.1370e-06 - val_loss: 0.0010\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 2.3319e-06 - val_loss: 0.0012\n",
      "Using 6 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.1632e-05 - val_loss: 2.9164e-04\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 4.0030e-06 - val_loss: 5.4898e-04\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.9818e-06 - val_loss: 3.6838e-04\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 9.6078e-06 - val_loss: 5.4081e-04\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 6.8309e-06 - val_loss: 2.1810e-04\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 5.2817e-06 - val_loss: 3.6430e-04\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.6559e-06 - val_loss: 2.6508e-04\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.3236e-06 - val_loss: 4.0279e-04\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.8664e-06 - val_loss: 2.2192e-04\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 4.0795e-06 - val_loss: 2.4184e-04\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.6467e-06 - val_loss: 5.2269e-04\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.7669e-06 - val_loss: 2.0698e-04\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.7240e-06 - val_loss: 3.9070e-04\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.8806e-06 - val_loss: 2.4206e-04\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 4.8616e-06 - val_loss: 2.8659e-04\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.8538e-06 - val_loss: 5.2516e-04\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 3.0882e-06 - val_loss: 6.5818e-04\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.7605e-06 - val_loss: 5.3172e-04\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.1590e-06 - val_loss: 2.6939e-04\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 3.1809e-06 - val_loss: 2.8115e-04\n",
      "Using 7 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 7.4369e-05 - val_loss: 2.6777e-04\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 2.6704e-06 - val_loss: 2.7037e-04\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 6.2148e-06 - val_loss: 3.0137e-04\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.7776e-06 - val_loss: 2.3403e-04\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.4003e-06 - val_loss: 2.0186e-04\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 4.6233e-06 - val_loss: 2.4131e-04\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 6.2912e-06 - val_loss: 2.0502e-04\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 2s 6ms/step - loss: 2.3916e-06 - val_loss: 1.8573e-04\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 2s 7ms/step - loss: 1.0719e-05 - val_loss: 9.0407e-04\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.7090e-06 - val_loss: 1.8825e-04\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 2s 5ms/step - loss: 2.3364e-06 - val_loss: 1.9946e-04\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.3365e-06 - val_loss: 1.5836e-04\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.8333e-06 - val_loss: 1.5340e-04\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.5693e-06 - val_loss: 1.5152e-04\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.7102e-06 - val_loss: 1.8211e-04\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.9146e-06 - val_loss: 1.5760e-04\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.3098e-06 - val_loss: 1.5112e-04\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.0867e-06 - val_loss: 2.0050e-04\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.5234e-06 - val_loss: 1.7041e-04\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.8605e-06 - val_loss: 1.5743e-04\n",
      "Using 8 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 1.6486e-04 - val_loss: 8.8933e-04\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 1.0660e-05 - val_loss: 8.4284e-04\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.3861e-06 - val_loss: 9.7743e-04\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.5855e-06 - val_loss: 8.6492e-04\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.1811e-06 - val_loss: 0.0012\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.1965e-06 - val_loss: 8.9504e-04\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 8.4076e-06 - val_loss: 0.0010\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.7764e-06 - val_loss: 5.9229e-04\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.0034e-06 - val_loss: 9.8546e-04\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.6607e-06 - val_loss: 6.7464e-04\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 7.8999e-06 - val_loss: 7.5563e-04\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 4.2937e-06 - val_loss: 4.1280e-04\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.3117e-06 - val_loss: 9.7777e-04\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.4867e-06 - val_loss: 7.2733e-04\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.0769e-06 - val_loss: 5.9169e-04\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 4.2491e-06 - val_loss: 9.8599e-04\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.7607e-06 - val_loss: 4.9785e-04\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.8868e-06 - val_loss: 5.7906e-04\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.7839e-06 - val_loss: 3.5764e-04\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.6815e-06 - val_loss: 5.9218e-04\n",
      "Using 9 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 8.1165e-05 - val_loss: 5.2008e-04\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 7.3621e-06 - val_loss: 5.5686e-04\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 5.6255e-06 - val_loss: 7.8112e-04\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 6.1283e-06 - val_loss: 7.0658e-04\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 5.4584e-06 - val_loss: 6.4454e-04\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 6.9369e-06 - val_loss: 5.4926e-04\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 5.5149e-06 - val_loss: 6.7036e-04\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 8.2523e-06 - val_loss: 7.3806e-04\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 4.4723e-06 - val_loss: 6.8778e-04\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.6191e-06 - val_loss: 5.4536e-04\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 4.7381e-06 - val_loss: 5.4036e-04\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.6990e-06 - val_loss: 3.7990e-04\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 7.0362e-06 - val_loss: 5.7631e-04\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 4.9944e-06 - val_loss: 4.8070e-04\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.6336e-06 - val_loss: 3.9961e-04\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.4678e-06 - val_loss: 4.4721e-04\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.6620e-06 - val_loss: 3.9207e-04\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.5518e-06 - val_loss: 3.0141e-04\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 6.1332e-06 - val_loss: 5.2814e-04\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.2558e-06 - val_loss: 2.5411e-04\n",
      "Using 10 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 1.6587e-05 - val_loss: 4.9290e-04\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 5.8920e-06 - val_loss: 3.6569e-04\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.7048e-06 - val_loss: 2.0060e-04\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 4.3199e-06 - val_loss: 2.8565e-04\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 4.8810e-06 - val_loss: 2.0363e-04\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 4.3394e-06 - val_loss: 1.8096e-04\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.9121e-06 - val_loss: 1.6555e-04\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.1056e-06 - val_loss: 1.4537e-04\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 4.6298e-06 - val_loss: 2.1264e-04\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 3.3174e-06 - val_loss: 1.7067e-04\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.8024e-06 - val_loss: 1.3610e-04\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.8358e-06 - val_loss: 1.6031e-04\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.7850e-06 - val_loss: 1.2921e-04\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.9452e-06 - val_loss: 1.4829e-04\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 2.9186e-06 - val_loss: 1.3020e-04\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 3.1249e-06 - val_loss: 1.7662e-04\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.3596e-06 - val_loss: 2.4839e-04\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 2.7558e-06 - val_loss: 2.1808e-04\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 3.3769e-06 - val_loss: 1.3059e-04\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 1s 5ms/step - loss: 2.5618e-06 - val_loss: 1.8892e-04\n"
     ]
    }
   ],
   "source": [
    "trained_models = select_inputs(normalized_h, 2, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trained_models[2]['model'].predict(val_targets))\n",
    "model = trained_models[2]['model']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = {}\n",
    "for k, v in trained_models.items():\n",
    "    train_history = v['history']\n",
    "    loss = train_history['loss'][-1]\n",
    "    val_loss = train_history['val_loss'][-1]\n",
    "    model_stats[k] = {'inputs': k, 'loss': loss, 'val_loss': val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {'inputs': 2,\n",
       "  'loss': 2.141165850844118e-06,\n",
       "  'val_loss': 0.00549785140901804},\n",
       " 3: {'inputs': 3,\n",
       "  'loss': 2.041754669335205e-06,\n",
       "  'val_loss': 0.0019678568933159113},\n",
       " 4: {'inputs': 4,\n",
       "  'loss': 2.9772370453429176e-06,\n",
       "  'val_loss': 0.000743925163988024},\n",
       " 5: {'inputs': 5,\n",
       "  'loss': 2.33186847253819e-06,\n",
       "  'val_loss': 0.0012499623699113727},\n",
       " 6: {'inputs': 6,\n",
       "  'loss': 3.180888143106131e-06,\n",
       "  'val_loss': 0.0002811510639730841},\n",
       " 7: {'inputs': 7,\n",
       "  'loss': 2.860469521692721e-06,\n",
       "  'val_loss': 0.00015743222320452332},\n",
       " 8: {'inputs': 8,\n",
       "  'loss': 2.681479372768081e-06,\n",
       "  'val_loss': 0.0005921847186982632},\n",
       " 9: {'inputs': 9,\n",
       "  'loss': 3.2558198199694743e-06,\n",
       "  'val_loss': 0.00025410958915017545},\n",
       " 10: {'inputs': 10,\n",
       "  'loss': 2.5617505343689118e-06,\n",
       "  'val_loss': 0.00018892182561103255}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4003792280>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = []\n",
    "indices = []\n",
    "for k, v in model_stats.items():\n",
    "    indices.append(k)\n",
    "    val_loss.append(v['val_loss'])\n",
    "plt.plot(indices, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.774582794031957\n",
      "3 0.30187933724308375\n",
      "4 0.13798198364852757\n",
      "5 0.2057456970026745\n",
      "6 0.07601165598474344\n",
      "7 0.059444399735174305\n",
      "8 0.11766233932497659\n",
      "9 0.07239051760376336\n",
      "10 0.06366118926395448\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "close_min = history['Close'].min()\n",
    "close_max = history['Close'].max()\n",
    "for k in model_stats:\n",
    "    e = ((close_max - close_min) * model_stats[k]['val_loss'] + close_min)\n",
    "    vals.append(e)\n",
    "    print(k, e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
