{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOGLE STOCKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGL -- Google's ticker\n",
    "google = yf.Ticker('GOOGL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = google.history(period='max', interval='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08-19</th>\n",
       "      <td>50.050049</td>\n",
       "      <td>52.082081</td>\n",
       "      <td>48.028027</td>\n",
       "      <td>50.220219</td>\n",
       "      <td>44659000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-20</th>\n",
       "      <td>50.555557</td>\n",
       "      <td>54.594593</td>\n",
       "      <td>50.300301</td>\n",
       "      <td>54.209209</td>\n",
       "      <td>22834300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-23</th>\n",
       "      <td>55.430431</td>\n",
       "      <td>56.796795</td>\n",
       "      <td>54.579578</td>\n",
       "      <td>54.754753</td>\n",
       "      <td>18256100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-24</th>\n",
       "      <td>55.675674</td>\n",
       "      <td>55.855854</td>\n",
       "      <td>51.836838</td>\n",
       "      <td>52.487488</td>\n",
       "      <td>15247300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-25</th>\n",
       "      <td>52.532532</td>\n",
       "      <td>54.054054</td>\n",
       "      <td>51.991993</td>\n",
       "      <td>53.053055</td>\n",
       "      <td>9188600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30</th>\n",
       "      <td>1775.650024</td>\n",
       "      <td>1780.339966</td>\n",
       "      <td>1747.839966</td>\n",
       "      <td>1754.400024</td>\n",
       "      <td>1620900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>1766.660034</td>\n",
       "      <td>1821.719971</td>\n",
       "      <td>1763.030029</td>\n",
       "      <td>1795.359985</td>\n",
       "      <td>1868100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-02</th>\n",
       "      <td>1795.359985</td>\n",
       "      <td>1832.739990</td>\n",
       "      <td>1785.170044</td>\n",
       "      <td>1824.969971</td>\n",
       "      <td>1471200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-03</th>\n",
       "      <td>1820.540039</td>\n",
       "      <td>1843.829956</td>\n",
       "      <td>1817.000000</td>\n",
       "      <td>1821.839966</td>\n",
       "      <td>1236400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-04</th>\n",
       "      <td>1820.219971</td>\n",
       "      <td>1829.500000</td>\n",
       "      <td>1813.589966</td>\n",
       "      <td>1823.760010</td>\n",
       "      <td>1015300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4104 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Volume  \\\n",
       "Date                                                                       \n",
       "2004-08-19    50.050049    52.082081    48.028027    50.220219  44659000   \n",
       "2004-08-20    50.555557    54.594593    50.300301    54.209209  22834300   \n",
       "2004-08-23    55.430431    56.796795    54.579578    54.754753  18256100   \n",
       "2004-08-24    55.675674    55.855854    51.836838    52.487488  15247300   \n",
       "2004-08-25    52.532532    54.054054    51.991993    53.053055   9188600   \n",
       "...                 ...          ...          ...          ...       ...   \n",
       "2020-11-30  1775.650024  1780.339966  1747.839966  1754.400024   1620900   \n",
       "2020-12-01  1766.660034  1821.719971  1763.030029  1795.359985   1868100   \n",
       "2020-12-02  1795.359985  1832.739990  1785.170044  1824.969971   1471200   \n",
       "2020-12-03  1820.540039  1843.829956  1817.000000  1821.839966   1236400   \n",
       "2020-12-04  1820.219971  1829.500000  1813.589966  1823.760010   1015300   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "Date                                 \n",
       "2004-08-19          0           0.0  \n",
       "2004-08-20          0           0.0  \n",
       "2004-08-23          0           0.0  \n",
       "2004-08-24          0           0.0  \n",
       "2004-08-25          0           0.0  \n",
       "...               ...           ...  \n",
       "2020-11-30          0           0.0  \n",
       "2020-12-01          0           0.0  \n",
       "2020-12-02          0           0.0  \n",
       "2020-12-03          0           0.0  \n",
       "2020-12-04          0           0.0  \n",
       "\n",
       "[4104 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function that uses TimeseriesGenerator class to generate the training set with dividends info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_series(data, value_num):\n",
    "    close = data['Close']\n",
    "    dividends = data['Dividends']\n",
    "    tsg = TimeseriesGenerator(close, close,\n",
    "                              length=value_num,\n",
    "                              batch_size=len(close))\n",
    "    global_index = value_num\n",
    "    i, t = tsg[0]\n",
    "    has_dividends = np.zeros(len(i))\n",
    "    for b_row in range(len(t)):\n",
    "        assert(abs(t[b_row] - close[global_index]) <= 0.001)\n",
    "        has_dividends[b_row] = dividends[global_index] > 0            \n",
    "        global_index += 1\n",
    "    return np.concatenate((i, np.transpose([has_dividends])),\n",
    "                           axis=1), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_series(history, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing MinMax normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_min = history.min()\n",
    "normalized_h = (history - h_min) / (history.max() - h_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_series(normalized_h, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creates a neural network with a specified number of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n):\n",
    "    m = models.Sequential()\n",
    "    m.add(layers.Dense(64, activation='relu', input_shape=(n+1,)))\n",
    "    m.add(layers.Dense(64, activation='relu'))\n",
    "    m.add(layers.Dense(1))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = inputs[:-1000]\n",
    "val_inputs = inputs[-1000:]\n",
    "train_targets = targets[:-1000]\n",
    "val_targets = targets[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_inputs(data, start, end, epochs):\n",
    "    models = {}\n",
    "    for inputs in range(start, end+1):\n",
    "        print('Using {} inputs'.format(inputs))\n",
    "        model_inputs, targets = generate_series(data, inputs)\n",
    "        \n",
    "        train_inputs = model_inputs[:-1000]\n",
    "        val_inputs = model_inputs[-1000:]\n",
    "        train_targets = targets[:-1000]\n",
    "        val_targets = targets[-1000:]\n",
    "        \n",
    "        m = create_model(inputs)\n",
    "        print('Training')\n",
    "        m.compile(optimizer='adam', loss='mse') \n",
    "        h = m.fit(train_inputs, train_targets,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=32,\n",
    "                  validation_data=(val_inputs, val_targets))\n",
    "        model_info = {'model': m, 'history': h.history}\n",
    "        models[inputs] = model_info\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0033\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.9486e-05 - val_loss: 0.0031\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.7185e-05 - val_loss: 0.0030\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.7024e-05 - val_loss: 0.0030\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.7046e-05 - val_loss: 0.0029\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6841e-05 - val_loss: 0.0029\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6181e-05 - val_loss: 0.0029\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6375e-05 - val_loss: 0.0029\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6944e-05 - val_loss: 0.0029\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6984e-05 - val_loss: 0.0029\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.6475e-05 - val_loss: 0.0030\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 1.6596e-05 - val_loss: 0.0030\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 1.7749e-05 - val_loss: 0.0026\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6852e-05 - val_loss: 0.0028\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 1.6898e-05 - val_loss: 0.0033\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.6940e-05 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5647e-05 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6045e-05 - val_loss: 0.0028\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.5353e-05 - val_loss: 0.0028\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5807e-05 - val_loss: 0.0026\n",
      "Using 3 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0011\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 2.7555e-05 - val_loss: 7.2535e-04\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.5055e-05 - val_loss: 7.3919e-04\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 2.4063e-05 - val_loss: 6.1411e-04\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 2.2895e-05 - val_loss: 6.7959e-04\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.1919e-05 - val_loss: 6.7081e-04\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.1505e-05 - val_loss: 6.0551e-04\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.2413e-05 - val_loss: 6.1231e-04\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.9983e-05 - val_loss: 4.2744e-04\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.0750e-05 - val_loss: 8.1284e-04\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.9180e-05 - val_loss: 5.6473e-04\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.7810e-05 - val_loss: 4.9098e-04\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.9437e-05 - val_loss: 5.1030e-04\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.7068e-05 - val_loss: 5.5281e-04\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.8265e-05 - val_loss: 5.7464e-04\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.6626e-05 - val_loss: 4.9546e-04\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.8297e-05 - val_loss: 5.0070e-04\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5415e-05 - val_loss: 3.9974e-04\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.7873e-05 - val_loss: 5.9685e-04\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.7078e-05 - val_loss: 4.5358e-04\n",
      "Using 4 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 3.4775e-04\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.6805e-05 - val_loss: 2.7885e-04\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 2.5836e-05 - val_loss: 2.7560e-04\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 2.4551e-05 - val_loss: 2.3917e-04\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 2.3854e-05 - val_loss: 2.2389e-04\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.3539e-05 - val_loss: 2.1682e-04\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.2098e-05 - val_loss: 2.2567e-04\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 2.1129e-05 - val_loss: 1.9144e-04\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.0534e-05 - val_loss: 1.8535e-04\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 2.1563e-05 - val_loss: 2.0729e-04\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 1.8638e-05 - val_loss: 1.7656e-04\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.0064e-05 - val_loss: 2.0615e-04\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.9845e-05 - val_loss: 2.2611e-04\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.9855e-05 - val_loss: 2.0823e-04\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.8350e-05 - val_loss: 2.1577e-04\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.8372e-05 - val_loss: 1.8011e-04\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.8536e-05 - val_loss: 1.8743e-04\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.7886e-05 - val_loss: 1.6385e-04\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.8654e-05 - val_loss: 1.6349e-04\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.7245e-05 - val_loss: 2.9664e-04\n",
      "Using 5 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 0.0043 - val_loss: 9.2094e-04\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9823e-05 - val_loss: 0.0010\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.8888e-05 - val_loss: 0.0010\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.8484e-05 - val_loss: 8.6752e-04\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 1.7623e-05 - val_loss: 0.0010\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.7696e-05 - val_loss: 8.3914e-04\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.6719e-05 - val_loss: 7.3722e-04\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.6675e-05 - val_loss: 8.0894e-04\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.7303e-05 - val_loss: 6.8033e-04\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.6746e-05 - val_loss: 7.5951e-04\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.6586e-05 - val_loss: 8.5150e-04\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6823e-05 - val_loss: 6.4395e-04\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 1.5825e-05 - val_loss: 6.8417e-04\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.5321e-05 - val_loss: 6.1359e-04\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.5932e-05 - val_loss: 7.7748e-04\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.6024e-05 - val_loss: 6.2909e-04\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.6640e-05 - val_loss: 6.9752e-04\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.5275e-05 - val_loss: 5.5266e-04\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.5536e-05 - val_loss: 6.4751e-04\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.6324e-05 - val_loss: 5.0871e-04\n",
      "Using 6 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3196e-04 - val_loss: 2.8455e-04\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 3.0833e-05 - val_loss: 3.4204e-04\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 3.0251e-05 - val_loss: 2.6500e-04\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.7918e-05 - val_loss: 2.5381e-04\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.6099e-05 - val_loss: 2.2811e-04\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.6548e-05 - val_loss: 3.5960e-04\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.3928e-05 - val_loss: 1.9162e-04\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.2302e-05 - val_loss: 1.7995e-04\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.2679e-05 - val_loss: 1.7679e-04\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.9643e-05 - val_loss: 1.9065e-04\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.8922e-05 - val_loss: 1.9147e-04\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.4192e-05 - val_loss: 1.8801e-04\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 1.9010e-05 - val_loss: 1.6017e-04\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.6416e-05 - val_loss: 1.5749e-04\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.7905e-05 - val_loss: 1.8260e-04\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.6853e-05 - val_loss: 1.6748e-04\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.6410e-05 - val_loss: 1.4748e-04\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.7033e-05 - val_loss: 2.1770e-04\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.1196e-05 - val_loss: 1.5998e-04\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 1.8076e-05 - val_loss: 1.4730e-04\n",
      "Using 7 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.0017 - val_loss: 4.7906e-04\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 3.6953e-05 - val_loss: 6.2238e-04\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 3.4944e-05 - val_loss: 5.5630e-04\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 3.3710e-05 - val_loss: 3.7982e-04\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 3.1405e-05 - val_loss: 3.3105e-04\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.9102e-05 - val_loss: 3.0931e-04\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.7708e-05 - val_loss: 4.0476e-04\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.7347e-05 - val_loss: 2.6693e-04\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.3562e-05 - val_loss: 2.3435e-04\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.2598e-05 - val_loss: 2.4946e-04\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.1534e-05 - val_loss: 2.0737e-04\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.1353e-05 - val_loss: 2.0476e-04\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.9051e-05 - val_loss: 1.9930e-04\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.9464e-05 - val_loss: 1.7595e-04\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.0690e-05 - val_loss: 2.0370e-04\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.8867e-05 - val_loss: 1.7560e-04\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.8719e-05 - val_loss: 3.0479e-04\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.0531e-05 - val_loss: 1.6398e-04\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 1.7758e-05 - val_loss: 1.6146e-04\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.7590e-05 - val_loss: 1.6274e-04\n",
      "Using 8 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.0016 - val_loss: 3.4289e-04\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 3.7613e-05 - val_loss: 3.1827e-04\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 3.3908e-05 - val_loss: 2.8876e-04\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.7453e-05 - val_loss: 2.4541e-04\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.5459e-05 - val_loss: 2.4453e-04\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.5625e-05 - val_loss: 2.2571e-04\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.4651e-05 - val_loss: 4.0358e-04\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.5488e-05 - val_loss: 2.0913e-04\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.1619e-05 - val_loss: 2.1683e-04\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.1226e-05 - val_loss: 1.9317e-04\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.1712e-05 - val_loss: 1.8978e-04\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.2618e-05 - val_loss: 2.1844e-04\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.0445e-05 - val_loss: 1.9943e-04\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.1269e-05 - val_loss: 1.9034e-04\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.0447e-05 - val_loss: 2.5077e-04\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.0419e-05 - val_loss: 1.8821e-04\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.9558e-05 - val_loss: 1.8704e-04\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 2.0543e-05 - val_loss: 2.1519e-04\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.1133e-05 - val_loss: 1.9069e-04\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.8487e-05 - val_loss: 6.4963e-04\n",
      "Using 9 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 0.0054 - val_loss: 0.0013\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 6.0000e-05 - val_loss: 0.0014\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 5.2413e-05 - val_loss: 0.0011\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 4.3520e-05 - val_loss: 0.0011\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 3.4048e-05 - val_loss: 9.9372e-04\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.9519e-05 - val_loss: 7.3712e-04\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.9798e-05 - val_loss: 8.6340e-04\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.5729e-05 - val_loss: 6.3461e-04\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.7059e-05 - val_loss: 6.1814e-04\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.5279e-05 - val_loss: 7.9497e-04\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.4679e-05 - val_loss: 5.1588e-04\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.2123e-05 - val_loss: 6.4818e-04\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.3539e-05 - val_loss: 6.6328e-04\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.3037e-05 - val_loss: 7.2271e-04\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.1241e-05 - val_loss: 6.0705e-04\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.1052e-05 - val_loss: 4.7817e-04\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.1996e-05 - val_loss: 5.6101e-04\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9798e-05 - val_loss: 5.6379e-04\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.1148e-05 - val_loss: 4.8241e-04\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.3292e-05 - val_loss: 9.4064e-04\n",
      "Using 10 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 4.8099e-04\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 4.1561e-05 - val_loss: 4.2395e-04\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 3.8502e-05 - val_loss: 4.8925e-04\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 3.3863e-05 - val_loss: 3.4396e-04\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.8228e-05 - val_loss: 3.3696e-04\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.7265e-05 - val_loss: 2.6942e-04\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.4566e-05 - val_loss: 2.6828e-04\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.6545e-05 - val_loss: 3.1981e-04\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 2.2943e-05 - val_loss: 2.3832e-04\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.2464e-05 - val_loss: 3.2675e-04\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.0596e-05 - val_loss: 2.2598e-04\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.1964e-05 - val_loss: 2.6732e-04\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 1.9548e-05 - val_loss: 2.2728e-04\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 1.8682e-05 - val_loss: 2.7634e-04\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.2703e-05 - val_loss: 2.3219e-04\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.2721e-05 - val_loss: 2.6797e-04\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 2.1841e-05 - val_loss: 2.1393e-04\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 1.8876e-05 - val_loss: 2.0242e-04\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.2145e-05 - val_loss: 1.7698e-04\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.5080e-05 - val_loss: 1.8525e-04\n"
     ]
    }
   ],
   "source": [
    "trained_models = select_inputs(normalized_h, 2, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trained_models[2]['model'].predict(val_targets))\n",
    "model = trained_models[2]['model']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = {}\n",
    "for k, v in trained_models.items():\n",
    "    train_history = v['history']\n",
    "    loss = train_history['loss'][-1]\n",
    "    val_loss = train_history['val_loss'][-1]\n",
    "    model_stats[k] = {'inputs': k, 'loss': loss, 'val_loss': val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {'inputs': 2,\n",
       "  'loss': 1.5807037925696932e-05,\n",
       "  'val_loss': 0.002550558652728796},\n",
       " 3: {'inputs': 3,\n",
       "  'loss': 1.707848241494503e-05,\n",
       "  'val_loss': 0.00045358389616012573},\n",
       " 4: {'inputs': 4,\n",
       "  'loss': 1.724522553558927e-05,\n",
       "  'val_loss': 0.00029664288740605116},\n",
       " 5: {'inputs': 5,\n",
       "  'loss': 1.6323729141731746e-05,\n",
       "  'val_loss': 0.000508710858412087},\n",
       " 6: {'inputs': 6,\n",
       "  'loss': 1.8076230844599195e-05,\n",
       "  'val_loss': 0.0001472986041335389},\n",
       " 7: {'inputs': 7,\n",
       "  'loss': 1.7590373317943886e-05,\n",
       "  'val_loss': 0.00016274039808195084},\n",
       " 8: {'inputs': 8,\n",
       "  'loss': 2.8486630981205963e-05,\n",
       "  'val_loss': 0.0006496281712315977},\n",
       " 9: {'inputs': 9,\n",
       "  'loss': 2.3291800971492194e-05,\n",
       "  'val_loss': 0.0009406386525370181},\n",
       " 10: {'inputs': 10,\n",
       "  'loss': 2.508047327864915e-05,\n",
       "  'val_loss': 0.00018525327323004603}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3bf431f1f0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = []\n",
    "indices = []\n",
    "for k, v in model_stats.items():\n",
    "    indices.append(k)\n",
    "    val_loss.append(v['val_loss'])\n",
    "plt.plot(indices, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 54.58207831032934\n",
      "3 50.86012653433954\n",
      "4 50.581569596814134\n",
      "5 50.957972201969014\n",
      "6 50.31649620066624\n",
      "7 50.3439040710904\n",
      "8 51.20808844255482\n",
      "9 51.724607286824885\n",
      "10 50.38386250901514\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "close_min = history['Close'].min()\n",
    "close_max = history['Close'].max()\n",
    "for k in model_stats:\n",
    "    e = ((close_max - close_min) * model_stats[k]['val_loss'] + close_min)\n",
    "    vals.append(e)\n",
    "    print(k, e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
