{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "from keras import models, layers\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apple's Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAPL -- Apple's ticker\n",
    "apple = yf.Ticker('AAPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = apple.history(period='max', interval='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-12-12</th>\n",
       "      <td>0.100266</td>\n",
       "      <td>0.100702</td>\n",
       "      <td>0.100266</td>\n",
       "      <td>0.100266</td>\n",
       "      <td>469033600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-15</th>\n",
       "      <td>0.095470</td>\n",
       "      <td>0.095470</td>\n",
       "      <td>0.095035</td>\n",
       "      <td>0.095035</td>\n",
       "      <td>175884800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-16</th>\n",
       "      <td>0.088495</td>\n",
       "      <td>0.088495</td>\n",
       "      <td>0.088059</td>\n",
       "      <td>0.088059</td>\n",
       "      <td>105728000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-17</th>\n",
       "      <td>0.090239</td>\n",
       "      <td>0.090675</td>\n",
       "      <td>0.090239</td>\n",
       "      <td>0.090239</td>\n",
       "      <td>86441600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-18</th>\n",
       "      <td>0.092855</td>\n",
       "      <td>0.093291</td>\n",
       "      <td>0.092855</td>\n",
       "      <td>0.092855</td>\n",
       "      <td>73449600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02</th>\n",
       "      <td>108.922083</td>\n",
       "      <td>110.489379</td>\n",
       "      <td>107.135165</td>\n",
       "      <td>108.582664</td>\n",
       "      <td>122866900</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>109.471139</td>\n",
       "      <td>111.297981</td>\n",
       "      <td>108.542740</td>\n",
       "      <td>110.249794</td>\n",
       "      <td>107624400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-04</th>\n",
       "      <td>113.943419</td>\n",
       "      <td>115.390919</td>\n",
       "      <td>112.156501</td>\n",
       "      <td>114.752022</td>\n",
       "      <td>138235500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05</th>\n",
       "      <td>117.746855</td>\n",
       "      <td>119.413985</td>\n",
       "      <td>116.668721</td>\n",
       "      <td>118.824997</td>\n",
       "      <td>126387100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-06</th>\n",
       "      <td>118.320000</td>\n",
       "      <td>119.199997</td>\n",
       "      <td>116.129997</td>\n",
       "      <td>118.690002</td>\n",
       "      <td>114283600</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10062 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close     Volume  \\\n",
       "Date                                                                    \n",
       "1980-12-12    0.100266    0.100702    0.100266    0.100266  469033600   \n",
       "1980-12-15    0.095470    0.095470    0.095035    0.095035  175884800   \n",
       "1980-12-16    0.088495    0.088495    0.088059    0.088059  105728000   \n",
       "1980-12-17    0.090239    0.090675    0.090239    0.090239   86441600   \n",
       "1980-12-18    0.092855    0.093291    0.092855    0.092855   73449600   \n",
       "...                ...         ...         ...         ...        ...   \n",
       "2020-11-02  108.922083  110.489379  107.135165  108.582664  122866900   \n",
       "2020-11-03  109.471139  111.297981  108.542740  110.249794  107624400   \n",
       "2020-11-04  113.943419  115.390919  112.156501  114.752022  138235500   \n",
       "2020-11-05  117.746855  119.413985  116.668721  118.824997  126387100   \n",
       "2020-11-06  118.320000  119.199997  116.129997  118.690002  114283600   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "Date                                 \n",
       "1980-12-12      0.000           0.0  \n",
       "1980-12-15      0.000           0.0  \n",
       "1980-12-16      0.000           0.0  \n",
       "1980-12-17      0.000           0.0  \n",
       "1980-12-18      0.000           0.0  \n",
       "...               ...           ...  \n",
       "2020-11-02      0.000           0.0  \n",
       "2020-11-03      0.000           0.0  \n",
       "2020-11-04      0.000           0.0  \n",
       "2020-11-05      0.000           0.0  \n",
       "2020-11-06      0.205           0.0  \n",
       "\n",
       "[10062 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function that uses TimeseriesGenerator class to generate the training set with dividends info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_series(data, value_num):\n",
    "    close = data['Close']\n",
    "    dividends = data['Dividends']\n",
    "    tsg = TimeseriesGenerator(close, close,\n",
    "                              length=value_num,\n",
    "                              batch_size=len(close))\n",
    "    global_index = value_num\n",
    "    i, t = tsg[0]\n",
    "    has_dividends = np.zeros(len(i))\n",
    "    for b_row in range(len(t)):\n",
    "        assert(abs(t[b_row] - close[global_index]) <= 0.001)\n",
    "        has_dividends[b_row] = dividends[global_index] > 0            \n",
    "        global_index += 1\n",
    "    return np.concatenate((i, np.transpose([has_dividends])),\n",
    "                           axis=1), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_series(history, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing MinMax normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_min = history.min()\n",
    "normalized_h = (history - h_min) / (history.max() - h_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = generate_series(normalized_h, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creates a neural network with a specified number of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n):\n",
    "    m = models.Sequential()\n",
    "    m.add(layers.Dense(64, activation='relu', input_shape=(n+1,)))\n",
    "    m.add(layers.Dense(64, activation='relu'))\n",
    "    m.add(layers.Dense(1))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = inputs[:-1000]\n",
    "val_inputs = inputs[-1000:]\n",
    "train_targets = targets[:-1000]\n",
    "val_targets = targets[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_inputs(data, start, end, epochs):\n",
    "    models = {}\n",
    "    for inputs in range(start, end+1):\n",
    "        print('Using {} inputs'.format(inputs))\n",
    "        model_inputs, targets = generate_series(data, inputs)\n",
    "        \n",
    "        train_inputs = model_inputs[:-1000]\n",
    "        val_inputs = model_inputs[-1000:]\n",
    "        train_targets = targets[:-1000]\n",
    "        val_targets = targets[-1000:]\n",
    "        \n",
    "        m = create_model(inputs)\n",
    "        print('Training')\n",
    "        m.compile(optimizer='adam', loss='mse') \n",
    "        h = m.fit(train_inputs, train_targets,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=32,\n",
    "                  validation_data=(val_inputs, val_targets))\n",
    "        model_info = {'model': m, 'history': h.history}\n",
    "        models[inputs] = model_info\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 8.7534e-05 - val_loss: 0.0055\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 5.1700e-06 - val_loss: 0.0051\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 0s 889us/step - loss: 1.9065e-06 - val_loss: 0.0051\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 0s 910us/step - loss: 3.6403e-06 - val_loss: 0.0055\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 0s 871us/step - loss: 2.9702e-06 - val_loss: 0.0052\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 0s 852us/step - loss: 1.9016e-06 - val_loss: 0.0053\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 0s 884us/step - loss: 8.1460e-06 - val_loss: 0.0050\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.1385e-05 - val_loss: 0.0027\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 0s 905us/step - loss: 1.7288e-06 - val_loss: 0.0025\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 0s 893us/step - loss: 1.6212e-06 - val_loss: 0.0025\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 0s 885us/step - loss: 1.8215e-06 - val_loss: 0.0029\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 0s 919us/step - loss: 1.8236e-06 - val_loss: 0.0023\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 0s 912us/step - loss: 1.7151e-06 - val_loss: 0.0024\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 0s 854us/step - loss: 1.7534e-06 - val_loss: 0.0025\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 0s 871us/step - loss: 1.9484e-06 - val_loss: 0.0023\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 0s 901us/step - loss: 1.8292e-06 - val_loss: 0.0028\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 0s 865us/step - loss: 2.9049e-06 - val_loss: 0.0026\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 0s 948us/step - loss: 2.5225e-06 - val_loss: 0.0021\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 0s 924us/step - loss: 2.2256e-06 - val_loss: 0.0022\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 0s 908us/step - loss: 2.2066e-06 - val_loss: 0.0022\n",
      "Using 3 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.7166e-04 - val_loss: 0.0038\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 0s 951us/step - loss: 6.2427e-06 - val_loss: 0.0043\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 0s 914us/step - loss: 1.0944e-05 - val_loss: 0.0036\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 0s 949us/step - loss: 1.7305e-06 - val_loss: 0.0039\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.9394e-06 - val_loss: 0.0036\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.5414e-05 - val_loss: 0.0027\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.7810e-06 - val_loss: 0.0031\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.0949e-06 - val_loss: 0.0019\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 0s 950us/step - loss: 1.0801e-05 - val_loss: 0.0033\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 0s 867us/step - loss: 5.5462e-06 - val_loss: 0.0021\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.8253e-06 - val_loss: 0.0018\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 0s 877us/step - loss: 1.8509e-06 - val_loss: 0.0022\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 0s 898us/step - loss: 1.8533e-06 - val_loss: 0.0019\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 0s 847us/step - loss: 2.2306e-06 - val_loss: 0.0020\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 0s 811us/step - loss: 1.2054e-05 - val_loss: 0.0014\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 0s 830us/step - loss: 1.7369e-06 - val_loss: 0.0019\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 0s 824us/step - loss: 1.7104e-06 - val_loss: 0.0015\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 0s 819us/step - loss: 2.0430e-06 - val_loss: 0.0014\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 0s 837us/step - loss: 1.9691e-06 - val_loss: 0.0016\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 0s 918us/step - loss: 2.1510e-06 - val_loss: 0.0015\n",
      "Using 4 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.5559e-04 - val_loss: 0.0026\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 0s 874us/step - loss: 2.5676e-06 - val_loss: 0.0027\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 0s 941us/step - loss: 3.2594e-06 - val_loss: 0.0026\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 0s 926us/step - loss: 2.6872e-06 - val_loss: 0.0025\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 0s 820us/step - loss: 1.0076e-05 - val_loss: 0.0017\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 0s 791us/step - loss: 2.5784e-06 - val_loss: 0.0019\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 0s 921us/step - loss: 2.5483e-06 - val_loss: 0.0014\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 0s 858us/step - loss: 2.5428e-06 - val_loss: 0.0018\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 0s 897us/step - loss: 2.5724e-06 - val_loss: 0.0017\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 0s 955us/step - loss: 2.9115e-06 - val_loss: 0.0018\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 0s 883us/step - loss: 2.7782e-06 - val_loss: 0.0014\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 0s 904us/step - loss: 3.1272e-06 - val_loss: 0.0013\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 0s 841us/step - loss: 4.5102e-06 - val_loss: 0.0018\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 0s 908us/step - loss: 2.8165e-06 - val_loss: 0.0016\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.2766e-06 - val_loss: 0.0019\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 0s 908us/step - loss: 2.6958e-06 - val_loss: 0.0017\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 0s 820us/step - loss: 2.5826e-06 - val_loss: 0.0020\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 0s 924us/step - loss: 2.0243e-06 - val_loss: 0.0013\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 0s 925us/step - loss: 2.4761e-06 - val_loss: 0.0017\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 5.8524e-06 - val_loss: 9.6422e-04\n",
      "Using 5 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 4.9568e-05 - val_loss: 2.4704e-04\n",
      "Epoch 2/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.0091e-06 - val_loss: 2.3606e-04\n",
      "Epoch 3/20\n",
      "284/284 [==============================] - 0s 975us/step - loss: 1.7948e-05 - val_loss: 0.0018\n",
      "Epoch 4/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.8761e-06 - val_loss: 1.6641e-04\n",
      "Epoch 5/20\n",
      "284/284 [==============================] - 0s 912us/step - loss: 2.6557e-06 - val_loss: 1.6048e-04\n",
      "Epoch 6/20\n",
      "284/284 [==============================] - 0s 966us/step - loss: 2.9042e-06 - val_loss: 1.8043e-04\n",
      "Epoch 7/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.0782e-06 - val_loss: 1.5786e-04\n",
      "Epoch 8/20\n",
      "284/284 [==============================] - 0s 928us/step - loss: 2.7928e-06 - val_loss: 1.6180e-04\n",
      "Epoch 9/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 5.9746e-06 - val_loss: 2.2511e-04\n",
      "Epoch 10/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 9.1356e-06 - val_loss: 1.6099e-04\n",
      "Epoch 11/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.2473e-06 - val_loss: 1.6651e-04\n",
      "Epoch 12/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.2030e-06 - val_loss: 2.2623e-04\n",
      "Epoch 13/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 2.6176e-06 - val_loss: 0.0020\n",
      "Epoch 14/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 8.6174e-06 - val_loss: 1.4769e-04\n",
      "Epoch 15/20\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 3.5086e-06 - val_loss: 2.0856e-04\n",
      "Epoch 16/20\n",
      "284/284 [==============================] - 0s 891us/step - loss: 2.9239e-06 - val_loss: 4.2241e-04\n",
      "Epoch 17/20\n",
      "284/284 [==============================] - 0s 957us/step - loss: 2.4405e-06 - val_loss: 1.4598e-04\n",
      "Epoch 18/20\n",
      "284/284 [==============================] - 0s 953us/step - loss: 6.8128e-06 - val_loss: 1.3348e-04\n",
      "Epoch 19/20\n",
      "284/284 [==============================] - 0s 907us/step - loss: 2.1115e-06 - val_loss: 1.2912e-04\n",
      "Epoch 20/20\n",
      "284/284 [==============================] - 0s 965us/step - loss: 2.0948e-06 - val_loss: 1.3151e-04\n",
      "Using 6 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 6.7838e-05 - val_loss: 2.8584e-04\n",
      "Epoch 2/20\n",
      "283/283 [==============================] - 0s 923us/step - loss: 2.9390e-06 - val_loss: 1.9507e-04\n",
      "Epoch 3/20\n",
      "283/283 [==============================] - 0s 918us/step - loss: 3.3512e-06 - val_loss: 1.7129e-04\n",
      "Epoch 4/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.5126e-06 - val_loss: 1.4823e-04\n",
      "Epoch 5/20\n",
      "283/283 [==============================] - 0s 951us/step - loss: 4.6714e-06 - val_loss: 1.9320e-04\n",
      "Epoch 6/20\n",
      "283/283 [==============================] - 0s 958us/step - loss: 3.2439e-06 - val_loss: 2.1325e-04\n",
      "Epoch 7/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 8.1529e-06 - val_loss: 7.7734e-04\n",
      "Epoch 8/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 4.3832e-06 - val_loss: 2.6574e-04\n",
      "Epoch 9/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.4740e-06 - val_loss: 2.1614e-04\n",
      "Epoch 10/20\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 3.6800e-06 - val_loss: 3.4472e-04\n",
      "Epoch 11/20\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 4.7763e-06 - val_loss: 2.4895e-04\n",
      "Epoch 12/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 1.9946e-06 - val_loss: 2.3832e-04\n",
      "Epoch 13/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.5873e-06 - val_loss: 1.6833e-04\n",
      "Epoch 14/20\n",
      "283/283 [==============================] - 0s 898us/step - loss: 3.3964e-06 - val_loss: 4.0042e-04\n",
      "Epoch 15/20\n",
      "283/283 [==============================] - 0s 909us/step - loss: 2.2968e-06 - val_loss: 2.6509e-04\n",
      "Epoch 16/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.1029e-06 - val_loss: 3.3284e-04\n",
      "Epoch 17/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.6522e-06 - val_loss: 2.4635e-04\n",
      "Epoch 18/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.3607e-06 - val_loss: 2.4608e-04\n",
      "Epoch 19/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.2236e-06 - val_loss: 2.7076e-04\n",
      "Epoch 20/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.0978e-06 - val_loss: 1.6656e-04\n",
      "Using 7 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 7.4574e-05 - val_loss: 5.5049e-04\n",
      "Epoch 2/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.6085e-06 - val_loss: 3.2547e-04\n",
      "Epoch 3/20\n",
      "283/283 [==============================] - 0s 974us/step - loss: 3.1632e-06 - val_loss: 3.3413e-04\n",
      "Epoch 4/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.3047e-06 - val_loss: 4.1047e-04\n",
      "Epoch 5/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 5.2608e-06 - val_loss: 4.1490e-04\n",
      "Epoch 6/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.7250e-06 - val_loss: 6.2384e-04\n",
      "Epoch 7/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.6593e-06 - val_loss: 3.8178e-04\n",
      "Epoch 8/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 4.4736e-06 - val_loss: 5.8602e-04\n",
      "Epoch 9/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.6372e-06 - val_loss: 3.9290e-04\n",
      "Epoch 10/20\n",
      "283/283 [==============================] - 0s 970us/step - loss: 2.5920e-06 - val_loss: 3.5948e-04\n",
      "Epoch 11/20\n",
      "283/283 [==============================] - 0s 960us/step - loss: 3.2913e-06 - val_loss: 3.9442e-04\n",
      "Epoch 12/20\n",
      "283/283 [==============================] - 0s 957us/step - loss: 3.1308e-06 - val_loss: 2.5425e-04\n",
      "Epoch 13/20\n",
      "283/283 [==============================] - 0s 960us/step - loss: 4.0514e-06 - val_loss: 2.4057e-04\n",
      "Epoch 14/20\n",
      "283/283 [==============================] - 0s 920us/step - loss: 3.3071e-06 - val_loss: 4.8159e-04\n",
      "Epoch 15/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.5878e-06 - val_loss: 3.8553e-04\n",
      "Epoch 16/20\n",
      "283/283 [==============================] - 0s 990us/step - loss: 2.6161e-06 - val_loss: 2.9773e-04\n",
      "Epoch 17/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.5950e-06 - val_loss: 3.0485e-04\n",
      "Epoch 18/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.3634e-06 - val_loss: 3.9114e-04\n",
      "Epoch 19/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.9903e-06 - val_loss: 1.9339e-04\n",
      "Epoch 20/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.0349e-06 - val_loss: 1.7244e-04\n",
      "Using 8 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 8.5143e-05 - val_loss: 4.4770e-04\n",
      "Epoch 2/20\n",
      "283/283 [==============================] - 0s 970us/step - loss: 3.2585e-06 - val_loss: 4.3838e-04\n",
      "Epoch 3/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 6.7839e-06 - val_loss: 4.4760e-04\n",
      "Epoch 4/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.2719e-06 - val_loss: 2.8283e-04\n",
      "Epoch 5/20\n",
      "283/283 [==============================] - 0s 966us/step - loss: 2.8004e-06 - val_loss: 3.1744e-04\n",
      "Epoch 6/20\n",
      "283/283 [==============================] - 0s 960us/step - loss: 3.8654e-06 - val_loss: 1.9940e-04\n",
      "Epoch 7/20\n",
      "283/283 [==============================] - 0s 950us/step - loss: 3.0890e-06 - val_loss: 2.9994e-04\n",
      "Epoch 8/20\n",
      "283/283 [==============================] - 0s 962us/step - loss: 5.6084e-06 - val_loss: 4.5680e-04\n",
      "Epoch 9/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.8570e-06 - val_loss: 2.1332e-04\n",
      "Epoch 10/20\n",
      "283/283 [==============================] - 0s 953us/step - loss: 2.5390e-06 - val_loss: 3.1093e-04\n",
      "Epoch 11/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.4722e-06 - val_loss: 2.2897e-04\n",
      "Epoch 12/20\n",
      "283/283 [==============================] - 0s 948us/step - loss: 2.3862e-06 - val_loss: 3.2707e-04\n",
      "Epoch 13/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.6399e-06 - val_loss: 4.0534e-04\n",
      "Epoch 14/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.7086e-06 - val_loss: 3.3852e-04\n",
      "Epoch 15/20\n",
      "283/283 [==============================] - 0s 980us/step - loss: 2.0544e-06 - val_loss: 2.4497e-04\n",
      "Epoch 16/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.4003e-06 - val_loss: 1.9019e-04\n",
      "Epoch 17/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.7038e-06 - val_loss: 2.6236e-04\n",
      "Epoch 18/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.3446e-06 - val_loss: 1.7163e-04\n",
      "Epoch 19/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.2189e-06 - val_loss: 1.4569e-04\n",
      "Epoch 20/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.3418e-06 - val_loss: 2.4059e-04\n",
      "Using 9 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 8.5483e-05 - val_loss: 4.6559e-04\n",
      "Epoch 2/20\n",
      "283/283 [==============================] - 0s 915us/step - loss: 6.8766e-06 - val_loss: 2.8390e-04\n",
      "Epoch 3/20\n",
      "283/283 [==============================] - 0s 959us/step - loss: 8.1057e-06 - val_loss: 3.3202e-04\n",
      "Epoch 4/20\n",
      "283/283 [==============================] - 0s 942us/step - loss: 9.2117e-06 - val_loss: 5.2484e-04\n",
      "Epoch 5/20\n",
      "283/283 [==============================] - 0s 965us/step - loss: 5.1594e-06 - val_loss: 2.6038e-04\n",
      "Epoch 6/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.9129e-06 - val_loss: 2.1830e-04\n",
      "Epoch 7/20\n",
      "283/283 [==============================] - 0s 953us/step - loss: 3.3996e-06 - val_loss: 3.3437e-04\n",
      "Epoch 8/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.7198e-06 - val_loss: 1.9661e-04\n",
      "Epoch 9/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 4.0011e-06 - val_loss: 4.5990e-04\n",
      "Epoch 10/20\n",
      "283/283 [==============================] - 0s 970us/step - loss: 8.4824e-06 - val_loss: 6.4032e-04\n",
      "Epoch 11/20\n",
      "283/283 [==============================] - 0s 960us/step - loss: 4.0195e-06 - val_loss: 2.2161e-04\n",
      "Epoch 12/20\n",
      "283/283 [==============================] - 0s 951us/step - loss: 2.4532e-06 - val_loss: 1.8031e-04\n",
      "Epoch 13/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 5.6052e-06 - val_loss: 3.5183e-04\n",
      "Epoch 14/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.1316e-06 - val_loss: 2.9138e-04\n",
      "Epoch 15/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.9089e-06 - val_loss: 3.8161e-04\n",
      "Epoch 16/20\n",
      "283/283 [==============================] - 0s 938us/step - loss: 2.7943e-06 - val_loss: 6.4789e-04\n",
      "Epoch 17/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 5.5342e-06 - val_loss: 4.2217e-04\n",
      "Epoch 18/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.2907e-06 - val_loss: 2.2712e-04\n",
      "Epoch 19/20\n",
      "283/283 [==============================] - 0s 974us/step - loss: 2.9289e-06 - val_loss: 2.7115e-04\n",
      "Epoch 20/20\n",
      "283/283 [==============================] - 0s 981us/step - loss: 3.1595e-06 - val_loss: 4.1013e-04\n",
      "Using 10 inputs\n",
      "Training\n",
      "Epoch 1/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 1.9959e-05 - val_loss: 4.3181e-04\n",
      "Epoch 2/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 6.1284e-06 - val_loss: 4.1988e-04\n",
      "Epoch 3/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 6.8076e-06 - val_loss: 6.7735e-04\n",
      "Epoch 4/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 5.6853e-06 - val_loss: 6.3519e-04\n",
      "Epoch 5/20\n",
      "283/283 [==============================] - 0s 971us/step - loss: 3.3084e-06 - val_loss: 4.1740e-04\n",
      "Epoch 6/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.1438e-06 - val_loss: 4.9458e-04\n",
      "Epoch 7/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.6380e-06 - val_loss: 2.6937e-04\n",
      "Epoch 8/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 5.3796e-06 - val_loss: 6.5474e-04\n",
      "Epoch 9/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.7074e-06 - val_loss: 3.4888e-04\n",
      "Epoch 10/20\n",
      "283/283 [==============================] - 0s 979us/step - loss: 3.0965e-06 - val_loss: 3.0808e-04\n",
      "Epoch 11/20\n",
      "283/283 [==============================] - 0s 970us/step - loss: 2.8989e-06 - val_loss: 4.5829e-04\n",
      "Epoch 12/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.9392e-06 - val_loss: 2.6853e-04\n",
      "Epoch 13/20\n",
      "283/283 [==============================] - 0s 915us/step - loss: 3.6758e-06 - val_loss: 3.6163e-04\n",
      "Epoch 14/20\n",
      "283/283 [==============================] - 0s 974us/step - loss: 2.8201e-06 - val_loss: 2.6555e-04\n",
      "Epoch 15/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 3.1950e-06 - val_loss: 2.8787e-04\n",
      "Epoch 16/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.7297e-06 - val_loss: 2.9983e-04\n",
      "Epoch 17/20\n",
      "283/283 [==============================] - 0s 927us/step - loss: 2.4446e-06 - val_loss: 3.4871e-04\n",
      "Epoch 18/20\n",
      "283/283 [==============================] - 0s 941us/step - loss: 3.9030e-06 - val_loss: 3.9566e-04\n",
      "Epoch 19/20\n",
      "283/283 [==============================] - 0s 968us/step - loss: 3.1241e-06 - val_loss: 2.9937e-04\n",
      "Epoch 20/20\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 2.4525e-06 - val_loss: 4.2342e-04\n"
     ]
    }
   ],
   "source": [
    "trained_models = select_inputs(normalized_h, 2, 10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = {}\n",
    "for k, v in trained_models.items():\n",
    "    train_history = v['history']\n",
    "    loss = train_history['loss'][-1]\n",
    "    val_loss = train_history['val_loss'][-1]\n",
    "    model_stats[k] = {'inputs': k, 'loss': loss, 'val_loss': val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {'inputs': 2,\n",
       "  'loss': 2.2065662506065564e-06,\n",
       "  'val_loss': 0.00217586406506598},\n",
       " 3: {'inputs': 3,\n",
       "  'loss': 2.150989303117967e-06,\n",
       "  'val_loss': 0.001493913703598082},\n",
       " 4: {'inputs': 4,\n",
       "  'loss': 5.852361937286332e-06,\n",
       "  'val_loss': 0.0009642249206081033},\n",
       " 5: {'inputs': 5,\n",
       "  'loss': 2.094831188514945e-06,\n",
       "  'val_loss': 0.00013151360326446593},\n",
       " 6: {'inputs': 6,\n",
       "  'loss': 3.0977651022112696e-06,\n",
       "  'val_loss': 0.00016655816580168903},\n",
       " 7: {'inputs': 7,\n",
       "  'loss': 3.0348744530783733e-06,\n",
       "  'val_loss': 0.00017243721231352538},\n",
       " 8: {'inputs': 8,\n",
       "  'loss': 2.3417665033775847e-06,\n",
       "  'val_loss': 0.0002405862760497257},\n",
       " 9: {'inputs': 9,\n",
       "  'loss': 3.1595388918503886e-06,\n",
       "  'val_loss': 0.0004101269587408751},\n",
       " 10: {'inputs': 10,\n",
       "  'loss': 2.4525334083591588e-06,\n",
       "  'val_loss': 0.0004234199586790055}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa3905f9550>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = []\n",
    "indices = []\n",
    "for k, v in model_stats.items():\n",
    "    indices.append(k)\n",
    "    val_loss.append(v['val_loss'])\n",
    "plt.plot(indices, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.3297336890517797\n",
      "3 0.23841335079335654\n",
      "4 0.16748244208290727\n",
      "5 0.05597362345312527\n",
      "6 0.06066645959757583\n",
      "7 0.061453725865669405\n",
      "8 0.07057960350138001\n",
      "9 0.09328288715181118\n",
      "10 0.09506295989523827\n"
     ]
    }
   ],
   "source": [
    "close_min = history['Close'].min()\n",
    "close_max = history['Close'].max()\n",
    "for k in model_stats:\n",
    "    e = ((close_max - close_min) * model_stats[k]['val_loss'] + close_min)\n",
    "    print(k, e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
